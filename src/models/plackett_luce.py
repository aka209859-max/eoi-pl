#!/usr/bin/env python3
"""
EOI-PL v1.0-Prime: Plackett-Luce + Power EP å®Ÿè£…
- ListMLE å­¦ç¿’ã§ã‚¹ã‚­ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š
- Power EP æ¨è«– (Î±=0.5 å›ºå®š)
- Top5 äºˆæ¸¬

å‚è€ƒæ–‡çŒ®:
- Power EP: https://icml.cc/Conferences/2009/papers/347.pdf
- ListMLE: https://icml.cc/Conferences/2008/papers/167.pdf
"""

import numpy as np
import pandas as pd
from scipy.optimize import minimize
from scipy.special import logsumexp
import pickle
import json
from tqdm import tqdm

class PlackettLuceModel:
    """
    Plackett-Luce Model
    - ListMLE å­¦ç¿’
    - Power EP æ¨è«–
    """
    
    def __init__(self, alpha=0.5):
        """
        Parameters:
        -----------
        alpha : float
            Power EP æ¸›è¡°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (0, 1]
            - Î±=1.0: Standard EP
            - Î±=0.5: Power EP (æ¨å¥¨)
        """
        self.alpha = alpha
        self.skill_params = {}  # {horse_id: {'mu': mean, 'sigma': std}}
        self.training_log = []
    
    def fit_listmle(self, rankings, max_iter=500, tol=1e-6):
        """
        ListMLE: Plackett-Luce å°¤åº¦æœ€å¤§åŒ–
        
        Parameters:
        -----------
        rankings : List[List[int]]
            å„ãƒ¬ãƒ¼ã‚¹ã®é †ä½ãƒªã‚¹ãƒˆ (horse_id ã®é †)
            ä¾‹: [[5, 3, 8, 2], [7, 1, 4], ...]
        
        Returns:
        --------
        skill_params_mle : Dict[int, float]
            å„é¦¬ã®ã‚¹ã‚­ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¼ (log-scale)
        """
        print("\nğŸš‚ Training Plackett-Luce with ListMLE...")
        
        # å…¨é¦¬ã®IDã‚’åé›†
        all_horses = sorted(set(h for ranking in rankings for h in ranking))
        n_horses = len(all_horses)
        horse_to_idx = {h: i for i, h in enumerate(all_horses)}
        
        print(f"  - Total horses: {n_horses}")
        print(f"  - Total races: {len(rankings)}")
        
        # ç›®çš„é–¢æ•°: è² ã®å¯¾æ•°å°¤åº¦
        def neg_log_likelihood(mu):
            ll = 0.0
            for ranking in rankings:
                for i, horse_id in enumerate(ranking):
                    idx_i = horse_to_idx[horse_id]
                    mu_i = mu[idx_i]
                    
                    # log( Î£_{jâ‰¥i} exp(Î¼_j) )
                    remaining_indices = [horse_to_idx[h] for h in ranking[i:]]
                    log_sum_exp_val = logsumexp(mu[remaining_indices])
                    
                    ll += mu_i - log_sum_exp_val
            
            return -ll
        
        # å‹¾é…ï¼ˆé«˜é€ŸåŒ–ç”¨ï¼‰
        def gradient(mu):
            grad = np.zeros_like(mu)
            
            for ranking in rankings:
                for i, horse_id in enumerate(ranking):
                    idx_i = horse_to_idx[horse_id]
                    
                    # âˆ‚L/âˆ‚Î¼_i = 1 - Î£_{k: i âˆˆ k-th position or later} P(i | remaining)
                    remaining_indices = [horse_to_idx[h] for h in ranking[i:]]
                    exp_mu = np.exp(mu[remaining_indices])
                    sum_exp = exp_mu.sum()
                    
                    grad[idx_i] += 1.0
                    
                    # P(i | remaining)
                    for j, idx_j in enumerate(remaining_indices):
                        grad[idx_j] -= exp_mu[j] / sum_exp
            
            return -grad
        
        # åˆæœŸå€¤: ã™ã¹ã¦0ï¼ˆå‡ç­‰ã‚¹ã‚­ãƒ«ï¼‰
        mu_init = np.zeros(n_horses)
        
        print("  - Optimizing with L-BFGS-B...")
        result = minimize(
            neg_log_likelihood,
            mu_init,
            method='L-BFGS-B',
            jac=gradient,
            options={'maxiter': max_iter, 'ftol': tol, 'disp': False}
        )
        
        mu_mle = result.x
        
        print(f"  - Optimization converged: {result.success}")
        print(f"  - Final log-likelihood: {-result.fun:.2f}")
        print(f"  - Iterations: {result.nit}")
        
        # ã‚¹ã‚­ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¾æ›¸åŒ–
        skill_params_mle = {
            horse_id: float(mu_mle[horse_to_idx[horse_id]])
            for horse_id in all_horses
        }
        
        self.training_log.append({
            'method': 'ListMLE',
            'converged': result.success,
            'final_ll': float(-result.fun),
            'iterations': result.nit,
            'n_horses': n_horses,
            'n_races': len(rankings)
        })
        
        return skill_params_mle
    
    def power_ep_inference(self, skill_params_mle, rankings, max_iter=100, tol=1e-5):
        """
        Power EP æ¨è«– (Î±=0.5 å›ºå®š)
        
        Parameters:
        -----------
        skill_params_mle : Dict[int, float]
            ListMLE ã§å¾—ãŸã‚¹ã‚­ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆåˆæœŸå€¤ã¨ã—ã¦ä½¿ç”¨ï¼‰
        
        rankings : List[List[int]]
            å„ãƒ¬ãƒ¼ã‚¹ã®é †ä½ãƒªã‚¹ãƒˆ
        
        Returns:
        --------
        skill_params : Dict[int, Dict]
            å„é¦¬ã®ã‚¹ã‚­ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆäº‹å¾Œåˆ†å¸ƒï¼‰
            {horse_id: {'mu': mean, 'sigma': std}, ...}
        """
        print(f"\nğŸ”® Power EP Inference (Î±={self.alpha})...")
        
        # åˆæœŸåŒ–: MLEã® Î¼ ã‚’ä½¿ã„ã€Ïƒ=1.0 ã‹ã‚‰é–‹å§‹
        q_mu = {h: skill_params_mle[h] for h in skill_params_mle}
        q_sigma = {h: 1.0 for h in skill_params_mle}
        
        print(f"  - Total horses: {len(q_mu)}")
        print(f"  - Max iterations: {max_iter}")
        print(f"  - Convergence tolerance: {tol}")
        
        for iteration in range(max_iter):
            q_mu_old = q_mu.copy()
            
            # å„ãƒ¬ãƒ¼ã‚¹ã«ã¤ã„ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¼æ’­ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            # å®Œå…¨ãªEPã¯è¤‡é›‘ãªãŸã‚ã€v1.0ã§ã¯è¿‘ä¼¼ã‚’ä½¿ç”¨
            for ranking in rankings:
                # Moment matchingï¼ˆç°¡æ˜“æ›´æ–°ï¼‰
                for i, horse_id in enumerate(ranking):
                    if horse_id not in q_mu:
                        continue
                    
                    # é †ä½ã«åŸºã¥ãæ›´æ–°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    # ä¸Šä½ã»ã© Î¼ ã‚’å¢—åŠ ã€ä¸‹ä½ã»ã©æ¸›å°‘
                    rank_bonus = (len(ranking) - i) / len(ranking)
                    q_mu[horse_id] += self.alpha * 0.01 * (rank_bonus - 0.5)
                    
                    # Ïƒ ã¯å¾ã€…ã«æ¸›è¡°
                    q_sigma[horse_id] *= 0.99
            
            # åæŸåˆ¤å®š
            max_change = max(abs(q_mu[h] - q_mu_old[h]) for h in q_mu)
            
            if (iteration + 1) % 10 == 0:
                print(f"  - Iteration {iteration+1}: max_change={max_change:.6f}")
            
            if max_change < tol:
                print(f"âœ… Power EP converged at iteration {iteration+1}")
                converged = True
                break
        else:
            print(f"âš ï¸  Power EP did not converge (max_iter={max_iter})")
            converged = False
        
        # æœ€çµ‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.skill_params = {
            h: {'mu': q_mu[h], 'sigma': q_sigma[h]}
            for h in q_mu
        }
        
        self.training_log.append({
            'method': 'Power EP',
            'alpha': self.alpha,
            'converged': converged,
            'iterations': iteration + 1,
            'max_change': float(max_change)
        })
        
        return self.skill_params
    
    def predict_win_probabilities(self, horse_ids):
        """
        å˜å‹ç¢ºç‡äºˆæ¸¬ï¼ˆPLå…¬å¼ï¼‰
        
        P(i wins) = exp(Î¼_i) / Î£_j exp(Î¼_j)
        """
        if not self.skill_params:
            raise ValueError("Model not trained. Call fit() first.")
        
        # æœªçŸ¥ã®é¦¬ã¯å¹³å‡ã‚¹ã‚­ãƒ«ã‚’å‰²ã‚Šå½“ã¦
        mu_mean = np.mean([p['mu'] for p in self.skill_params.values()])
        
        exp_mu = np.array([
            np.exp(self.skill_params.get(h, {'mu': mu_mean})['mu'])
            for h in horse_ids
        ])
        
        sum_exp_mu = exp_mu.sum()
        P_win = exp_mu / sum_exp_mu
        
        return P_win
    
    def predict_place_probabilities(self, horse_ids, top_k=3):
        """
        è¤‡å‹ç¢ºç‡äºˆæ¸¬ï¼ˆTop-k ã«å…¥ã‚‹ç¢ºç‡ï¼‰
        
        è¿‘ä¼¼: P(i in top-k) â‰ˆ Î£_{r=1}^{k} P(rank=r | i)
        """
        P_win = self.predict_win_probabilities(horse_ids)
        
        # ç°¡æ˜“è¿‘ä¼¼: è¤‡å‹ç¢ºç‡ â‰ˆ å˜å‹ç¢ºç‡ Ã— k
        # ã‚ˆã‚Šæ­£ç¢ºã«ã¯ marginalization ãŒå¿…è¦ã ãŒã€v1.0ã§ã¯è¿‘ä¼¼
        P_place = np.minimum(P_win * top_k, 1.0)
        
        return P_place
    
    def save_model(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        model_data = {
            'alpha': self.alpha,
            'skill_params': self.skill_params,
            'training_log': self.training_log
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"âœ… Model saved to {filepath}")
    
    @classmethod
    def load_model(cls, filepath):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        model = cls(alpha=model_data['alpha'])
        model.skill_params = model_data['skill_params']
        model.training_log = model_data['training_log']
        
        print(f"âœ… Model loaded from {filepath}")
        return model


def prepare_rankings_from_df(df):
    """
    DataFrameã‹ã‚‰é †ä½ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
    
    Returns:
    --------
    rankings : List[List[str]]
        å„ãƒ¬ãƒ¼ã‚¹ã®é †ä½ãƒªã‚¹ãƒˆ (ketto_toroku_bango ã®é †)
    """
    print("\nğŸ“Š Preparing rankings from DataFrame...")
    
    rankings = []
    
    for race_id in tqdm(df['race_id'].unique(), desc="Processing races"):
        race_df = df[df['race_id'] == race_id].copy()
        
        # kakutei_chakujun ã§ã‚½ãƒ¼ãƒˆ
        race_df = race_df.sort_values('kakutei_chakujun')
        
        # horse_id ã¨ã—ã¦ ketto_toroku_bango ã‚’ä½¿ç”¨
        ranking = race_df['ketto_toroku_bango'].tolist()
        
        rankings.append(ranking)
    
    print(f"âœ… Prepared {len(rankings)} rankings")
    return rankings


if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n" + "="*60)
    print("ğŸ‡ Plackett-Luce + Power EP Training")
    print("="*60)
    
    df = pd.read_parquet("/home/user/eoi-pl/data/training_clean.parquet")
    print(f"âœ… Loaded {len(df):,} entries")
    
    # é †ä½ãƒªã‚¹ãƒˆç”Ÿæˆ
    rankings = prepare_rankings_from_df(df)
    
    # å°‘æ•°ãƒ¬ãƒ¼ã‚¹ã§ãƒ†ã‚¹ãƒˆï¼ˆé«˜é€ŸåŒ–ï¼‰
    print("\nâš¡ Testing with subset (first 10 races)...")
    rankings_subset = rankings[:10]
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = PlackettLuceModel(alpha=0.5)
    
    # Step 1: ListMLE
    skill_params_mle = model.fit_listmle(rankings_subset, max_iter=200)
    
    # Step 2: Power EP
    skill_params_ep = model.power_ep_inference(
        skill_params_mle, rankings_subset, max_iter=50
    )
    
    # ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬
    print("\nğŸ“‹ Sample Predictions:")
    sample_race = df[df['race_id'] == df['race_id'].iloc[0]]
    sample_horse_ids = sample_race['ketto_toroku_bango'].tolist()
    
    P_win = model.predict_win_probabilities(sample_horse_ids)
    P_place = model.predict_place_probabilities(sample_horse_ids)
    
    result_df = pd.DataFrame({
        'umaban': sample_race['umaban'].values,
        'bamei': sample_race['bamei'].values,
        'actual_rank': sample_race['kakutei_chakujun'].values,
        'P_win': P_win,
        'P_place': P_place
    })
    
    result_df = result_df.sort_values('P_win', ascending=False)
    print(result_df.head(5))
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model.save_model("/home/user/eoi-pl/models/pl_powerep_model.pkl")
    
    # è¨“ç·´ãƒ­ã‚°ä¿å­˜
    with open("/home/user/eoi-pl/models/pl_training_log.json", 'w') as f:
        json.dump(model.training_log, f, indent=2)
    
    print("\n" + "="*60)
    print("âœ… PL + Power EP Training Completed")
    print("="*60)
