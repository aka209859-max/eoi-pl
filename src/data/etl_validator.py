#!/usr/bin/env python3
"""
EOI-PL v1.0-Prime: ETL & Forbiddenæ¤œå‡º
- odds/äººæ°—ç³»ã‚«ãƒ©ãƒ ã®æ¤œå‡º â†’ å³åœæ­¢
- DNF/ä¸­æ­¢/é™¤å¤–/å¤±æ ¼ã®é™¤å¤– (æ–¹é‡B)
- data_hash ç”Ÿæˆ
"""

import pandas as pd
import numpy as np
import psycopg2
import hashlib
import json
from datetime import datetime

class NumpyEncoder(json.JSONEncoder):
    """JSON encoder for numpy types"""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)

class DataValidator:
    """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    # ç¦æ­¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆCEOç¢ºå®šï¼‰
    FORBIDDEN_KEYWORDS = [
        'odds', 'ã‚ªãƒƒã‚º', 'ninki', 'äººæ°—', 'popularity',
        'tansho_odds', 'fukusho_odds', 'umaren_odds',
        'wide_odds', 'umatan_odds', 'sanrenpuku_odds', 'sanrentan_odds',
        'tansho_ninki', 'fukusho_ninki'
    ]
    
    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆDNF/ä¸­æ­¢/é™¤å¤–/å¤±æ ¼ï¼‰
    EXCLUSION_KEYWORDS = [
        'ä¸­æ­¢', 'é™¤å¤–', 'å¤±æ ¼', 'å–æ¶ˆ', 'ç«¶èµ°ä¸­æ­¢',
        'DQ', 'DNF', 'SCR', 'DISQ', 'å‡ºèµ°å–æ¶ˆ'
    ]
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—å€™è£œï¼ˆåˆ—åæºã‚Œå¯¾å¿œï¼‰
    STATUS_COLUMN_CANDIDATES = [
        'status', 'result_status', 'race_status', 'remarks',
        'disq_flag', 'joken', 'biko', 'memo'
    ]
    
    def __init__(self, conn):
        self.conn = conn
        self.exclusion_log = []
    
    def validate_no_forbidden_columns(self, df, source_name):
        """
        ç¦æ­¢ã‚«ãƒ©ãƒ æ¤œå‡º â†’ å³åœæ­¢
        """
        print(f"ğŸ” Validating {source_name} for forbidden columns...")
        
        forbidden_found = []
        for col in df.columns:
            for keyword in self.FORBIDDEN_KEYWORDS:
                if keyword.lower() in col.lower():
                    forbidden_found.append({
                        'column': col,
                        'keyword': keyword
                    })
        
        if forbidden_found:
            error_msg = f"ğŸš¨ FORBIDDEN COLUMNS DETECTED in {source_name}:\n"
            for item in forbidden_found:
                error_msg += f"  - Column: '{item['column']}' (matched: '{item['keyword']}')\n"
            error_msg += "\nâŒ SYSTEM HALTED for compliance violation."
            raise ValueError(error_msg)
        
        print(f"âœ… No forbidden columns in {source_name}")
    
    def detect_status_column(self, df):
        """
        ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ã‚’è‡ªå‹•æ¤œå‡ºï¼ˆåˆ—åæºã‚Œå¯¾å¿œï¼‰
        """
        for candidate in self.STATUS_COLUMN_CANDIDATES:
            if candidate in df.columns:
                print(f"âœ… Status column detected: '{candidate}'")
                return candidate
        
        print("âš ï¸  No status column found (will use missing rank only)")
        return None
    
    def exclude_dnf_and_disqualified(self, df):
        """
        DNF/ä¸­æ­¢/é™¤å¤–/å¤±æ ¼ã‚’é™¤å¤–ï¼ˆæ–¹é‡Bï¼‰
        
        é™¤å¤–æ¡ä»¶:
        1. kakutei_chakujun ãŒ NULL/0/è² ã®å€¤
        2. statusåˆ—ã«é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹
        
        Returns:
        --------
        df_clean: é™¤å¤–å¾Œã®DataFrame
        exclusion_audit: é™¤å¤–ç›£æŸ»æƒ…å ±
        """
        print("\nğŸ” Excluding DNF/disqualified entries...")
        
        original_count = len(df)
        exclusion_reasons = []
        
        # 1. ç€é †æ¬ æ/0/è² ã®å€¤ã‚’é™¤å¤–
        invalid_rank_mask = (
            df['kakutei_chakujun'].isnull() |
            (df['kakutei_chakujun'] <= 0)
        )
        
        if invalid_rank_mask.sum() > 0:
            excluded_races = df[invalid_rank_mask]['race_id'].unique()
            exclusion_reasons.append({
                'reason': 'missing_or_invalid_rank',
                'count': invalid_rank_mask.sum(),
                'sample_race_ids': excluded_races[:5].tolist()
            })
            print(f"  - Excluded {invalid_rank_mask.sum()} entries (missing/invalid rank)")
        
        df_clean = df[~invalid_rank_mask].copy()
        
        # 2. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ã«ã‚ˆã‚‹é™¤å¤–
        status_col = self.detect_status_column(df_clean)
        
        if status_col is not None:
            status_exclusion_mask = df_clean[status_col].apply(
                lambda x: any(kw in str(x) for kw in self.EXCLUSION_KEYWORDS) 
                if pd.notna(x) else False
            )
            
            if status_exclusion_mask.sum() > 0:
                excluded_races = df_clean[status_exclusion_mask]['race_id'].unique()
                exclusion_reasons.append({
                    'reason': f'status_keyword_match (column: {status_col})',
                    'count': status_exclusion_mask.sum(),
                    'sample_race_ids': excluded_races[:5].tolist(),
                    'keywords': self.EXCLUSION_KEYWORDS
                })
                print(f"  - Excluded {status_exclusion_mask.sum()} entries (status keywords)")
            
            df_clean = df_clean[~status_exclusion_mask].copy()
        
        final_count = len(df_clean)
        excluded_count = original_count - final_count
        
        print(f"âœ… Exclusion complete: {excluded_count}/{original_count} excluded")
        
        exclusion_audit = {
            'original_count': original_count,
            'excluded_count': excluded_count,
            'final_count': final_count,
            'exclusion_rate': excluded_count / original_count if original_count > 0 else 0.0,
            'reasons': exclusion_reasons
        }
        
        return df_clean, exclusion_audit
    
    def compute_data_hash(self, df):
        """
        ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ï¼ˆfreezeå†ç¾æ€§ä¿è¨¼ï¼‰
        """
        # race_id ã¨ umaban ã¨ kakutei_chakujun ã‚’ã‚½ãƒ¼ãƒˆã—ã¦ãƒãƒƒã‚·ãƒ¥åŒ–
        key_columns = ['race_id', 'umaban', 'kakutei_chakujun']
        df_sorted = df[key_columns].sort_values(key_columns).reset_index(drop=True)
        
        data_string = df_sorted.to_csv(index=False)
        data_hash = hashlib.sha256(data_string.encode()).hexdigest()
        
        print(f"âœ… Data hash: sha256:{data_hash[:16]}...")
        return f"sha256:{data_hash}"
    
    def load_and_validate_data(self, limit_date=None):
        """
        ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ + æ¤œè¨¼ + é™¤å¤–å‡¦ç†
        """
        print("\n" + "="*60)
        print("ğŸ“¥ Loading and validating data...")
        print("="*60)
        
        # ã‚¯ã‚¨ãƒªæ§‹ç¯‰
        query = """
            SELECT 
                r.race_id,
                r.kaisai_nen,
                r.kaisai_tsukihi,
                r.keibajo_code,
                r.race_bango,
                r.kyori,
                r.track_code,
                r.babajotai_code_dirt,
                r.kyoso_joken_code,
                r.tosu,
                e.umaban,
                e.wakuban,
                e.bamei,
                e.bataiju,
                e.kakutei_chakujun,
                e.soha_time,
                e.corner_1,
                e.corner_2,
                e.corner_3,
                e.corner_4,
                e.kohan_3f,
                e.ketto_toroku_bango,
                e.kishu_code,
                e.chokyoshi_code
            FROM races r
            INNER JOIN entries e ON r.race_id = e.race_id
        """
        
        # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã¯2024å¹´ã®ã¿ï¼ˆé«˜é€ŸåŒ–ï¼‰
        if limit_date:
            query += f" WHERE r.kaisai_nen = 2024"
        else:
            query += " WHERE r.kaisai_nen >= 2024"
        
        query += " ORDER BY r.kaisai_nen, r.kaisai_tsukihi, r.race_id, e.umaban"
        
        df = pd.read_sql(query, self.conn)
        print(f"âœ… Loaded {len(df):,} entries from database")
        
        # ç¦æ­¢ã‚«ãƒ©ãƒ æ¤œè¨¼
        self.validate_no_forbidden_columns(df, "database query result")
        
        # DNF/é™¤å¤–å‡¦ç†
        df_clean, exclusion_audit = self.exclude_dnf_and_disqualified(df)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        data_hash = self.compute_data_hash(df_clean)
        
        # ç›£æŸ»æƒ…å ±
        audit_info = {
            'data_hash': data_hash,
            'exclusion_audit': exclusion_audit,
            'data_quality': {
                'total_races': int(df_clean['race_id'].nunique()),
                'total_entries': int(len(df_clean)),
                'date_range': {
                    'min': int(df_clean['kaisai_nen'].min()),
                    'max': int(df_clean['kaisai_nen'].max())
                },
                'join_rate': 1.0,  # INNER JOIN ãªã®ã§ 100%
                'missing_rates': {
                    col: float(df_clean[col].isnull().mean())
                    for col in ['bataiju', 'corner_1', 'kohan_3f']
                }
            }
        }
        
        print(f"\nğŸ“Š Data Quality Summary:")
        print(f"  - Total races: {audit_info['data_quality']['total_races']:,}")
        print(f"  - Total entries: {audit_info['data_quality']['total_entries']:,}")
        print(f"  - Exclusion rate: {exclusion_audit['exclusion_rate']:.2%}")
        
        return df_clean, audit_info


if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    conn = psycopg2.connect(
        host="localhost",
        database="eoi_pl",
        user="postgres",
        password="eoi_pl_dev"
    )
    
    try:
        validator = DataValidator(conn)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ + æ¤œè¨¼ï¼ˆå­¦ç¿’ç”¨: 2024å¹´ã¾ã§ï¼‰
        df_train, audit_train = validator.load_and_validate_data(limit_date=20241231)
        
        # ä¿å­˜
        df_train.to_parquet("/home/user/eoi-pl/data/training_clean.parquet", index=False)
        
        with open("/home/user/eoi-pl/data/audit_etl.json", 'w') as f:
            json.dump(audit_train, f, indent=2, cls=NumpyEncoder)
        
        print("\n" + "="*60)
        print("âœ… ETL & Validation completed")
        print("="*60)
        print(f"  - Clean data: /home/user/eoi-pl/data/training_clean.parquet")
        print(f"  - Audit log:  /home/user/eoi-pl/data/audit_etl.json")
        
    finally:
        conn.close()
