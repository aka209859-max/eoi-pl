#!/usr/bin/env python3
"""
EOI-PL v1.0-Prime: Plackett-Luce + Power EP æœ€å°å®Ÿè£…
- å°‘æ•°ãƒ¬ãƒ¼ã‚¹ã§å‹•ä½œç¢ºèª
- Power EP (alpha=0.5) å®Ÿè£…
"""

import pandas as pd
import numpy as np
from scipy.special import logsumexp
from scipy.stats import norm
import pickle
import json
from datetime import datetime
from collections import defaultdict

class PowerEPPlackettLuce:
    """
    Plackett-Luce + Power EP (Î±=0.5)
    
    Reference: 
    Herbrich et al. (2006) "TrueSkill"
    Minka (2001) "Expectation Propagation"
    """
    
    def __init__(self, alpha=0.5, tau=0.0, beta=1.0):
        """
        Parameters:
        -----------
        alpha : float
            Power EPæ¸›è¡°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0.5æ¨å¥¨ï¼‰
        tau : float
            ã‚¹ã‚­ãƒ«äº‹å‰åˆ†å¸ƒã®å¹³å‡
        beta : float
            ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚¤ã‚º
        """
        self.alpha = alpha
        self.tau = tau
        self.beta = beta
        
        self.skill_mu = {}     # äº‹å¾Œå¹³å‡
        self.skill_sigma = {}  # äº‹å¾Œæ¨™æº–åå·®
        self.training_history = {}
    
    def fit_power_ep(self, rankings, max_iter=100, tol=1e-5):
        """
        Power EP ã«ã‚ˆã‚‹æ¨è«–
        
        Parameters:
        -----------
        rankings : List[Tuple[race_id, List[horse_id]]]
            å„ãƒ¬ãƒ¼ã‚¹ã®é †ä½ãƒªã‚¹ãƒˆ
        """
        print("\n" + "="*60)
        print(f"ğŸ”® Power EP Inference (Î±={self.alpha})")
        print("="*60)
        
        start_time = datetime.now()
        
        # å…¨é¦¬ã‚’åé›†
        all_horses = set()
        for race_id, ranking in rankings:
            all_horses.update(ranking)
        
        all_horses = sorted(all_horses)
        n_horses = len(all_horses)
        
        print(f"âœ… Races: {len(rankings)}")
        print(f"âœ… Horses: {n_horses}")
        
        # åˆæœŸåŒ–
        for h in all_horses:
            self.skill_mu[h] = self.tau
            self.skill_sigma[h] = 1.0
        
        # Power EP åå¾©
        converged = False
        for iteration in range(max_iter):
            mu_old = self.skill_mu.copy()
            
            # å„ãƒ¬ãƒ¼ã‚¹ã«ã¤ã„ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°
            for race_id, ranking in rankings:
                self._update_race(ranking)
            
            # åæŸåˆ¤å®š
            max_delta = max(abs(self.skill_mu[h] - mu_old[h]) for h in all_horses)
            
            if iteration % 10 == 0:
                print(f"  Iteration {iteration:3d}: max_delta = {max_delta:.6f}")
            
            if max_delta < tol:
                print(f"âœ… Converged at iteration {iteration}")
                converged = True
                break
        
        if not converged:
            print(f"âš ï¸  Did not converge after {max_iter} iterations (max_delta={max_delta:.6f})")
        
        end_time = datetime.now()
        training_time = (end_time - start_time).total_seconds()
        
        self.training_history = {
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'training_time_seconds': training_time,
            'n_races': len(rankings),
            'n_horses': n_horses,
            'power_ep': {
                'alpha': self.alpha,
                'iterations': iteration + 1,
                'converged': converged,
                'final_max_delta': float(max_delta),
                'convergence_criterion': tol
            }
        }
        
        print(f"\nğŸ“Š Training complete: {training_time:.2f}s")
        
        return self
    
    def _update_race(self, ranking):
        """
        1ãƒ¬ãƒ¼ã‚¹ã«ã¤ã„ã¦Power EPæ›´æ–°
        
        ç°¡æ˜“å®Ÿè£…: ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒã®ç©ã§è¿‘ä¼¼
        """
        n = len(ranking)
        
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ›´æ–°ï¼ˆi beats j for all i < jï¼‰
        for i in range(n):
            for j in range(i + 1, n):
                winner = ranking[i]
                loser = ranking[j]
                
                # Cavity distribution
                mu_w = self.skill_mu[winner]
                mu_l = self.skill_mu[loser]
                sigma_w = self.skill_sigma[winner]
                sigma_l = self.skill_sigma[loser]
                
                # Performanceå·®ã®åˆ†å¸ƒ
                mu_diff = mu_w - mu_l
                sigma_diff = np.sqrt(sigma_w**2 + sigma_l**2 + 2 * self.beta**2)
                
                # Truncated Gaussian momentsï¼ˆå‹è€…ãŒä¸Šä½ï¼‰
                v, w = self._truncated_gaussian_moments(mu_diff / sigma_diff)
                
                # Power EP update (Î±=0.5)
                delta_mu_w = self.alpha * sigma_w**2 / sigma_diff * v
                delta_mu_l = -self.alpha * sigma_l**2 / sigma_diff * v
                
                self.skill_mu[winner] += delta_mu_w
                self.skill_mu[loser] += delta_mu_l
                
                # åˆ†æ•£æ›´æ–°ï¼ˆç°¡æ˜“ï¼‰
                delta_sigma_w = -self.alpha * sigma_w**2 / sigma_diff**2 * w
                delta_sigma_l = -self.alpha * sigma_l**2 / sigma_diff**2 * w
                
                self.skill_sigma[winner] = max(0.1, self.skill_sigma[winner] + delta_sigma_w)
                self.skill_sigma[loser] = max(0.1, self.skill_sigma[loser] + delta_sigma_l)
    
    def _truncated_gaussian_moments(self, t):
        """
        æ¨™æº–æ­£è¦åˆ†å¸ƒã®åˆ‡æ–­ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ
        
        Returns:
        --------
        v : æœŸå¾…å€¤ã®è£œæ­£é …
        w : åˆ†æ•£ã®è£œæ­£é …
        """
        pdf = norm.pdf(t)
        cdf = norm.cdf(t)
        
        # æ•°å€¤å®‰å®šæ€§
        cdf = max(cdf, 1e-10)
        
        v = pdf / cdf
        w = v * (v + t)
        
        return v, w
    
    def predict_proba_pl(self, race_horses):
        """
        PLãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å˜å‹ç¢ºç‡äºˆæ¸¬
        
        P(horse_i wins) â‰ˆ exp(Î¼_i) / Î£_j exp(Î¼_j)
        """
        mu_values = np.array([
            self.skill_mu.get(h, self.tau) for h in race_horses
        ])
        
        # Softmax
        exp_mu = np.exp(mu_values)
        probs = exp_mu / exp_mu.sum()
        
        return {h: float(p) for h, p in zip(race_horses, probs)}
    
    def save(self, path):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        with open(path, 'wb') as f:
            pickle.dump({
                'alpha': self.alpha,
                'tau': self.tau,
                'beta': self.beta,
                'skill_mu': self.skill_mu,
                'skill_sigma': self.skill_sigma,
                'training_history': self.training_history
            }, f)
        print(f"âœ… Model saved: {path}")
    
    @classmethod
    def load(cls, path):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
        
        model = cls(alpha=data['alpha'], tau=data['tau'], beta=data['beta'])
        model.skill_mu = data['skill_mu']
        model.skill_sigma = data['skill_sigma']
        model.training_history = data['training_history']
        
        print(f"âœ… Model loaded: {path}")
        return model


if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("ğŸ“¥ Loading clean data...")
    df = pd.read_parquet("/home/user/eoi-pl/data/training_clean.parquet")
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæœ€å°å®Ÿè£…: æœ€æ–°100ãƒ¬ãƒ¼ã‚¹ï¼‰
    print("ğŸ”§ Sampling races for minimal implementation...")
    
    recent_races = df['race_id'].unique()[-100:]
    df_sample = df[df['race_id'].isin(recent_races)].copy()
    
    print(f"   Sample races: {len(recent_races)}")
    print(f"   Sample entries: {len(df_sample)}")
    
    # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«é †ä½ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
    rankings = []
    for race_id, race_df in df_sample.groupby('race_id'):
        race_df_sorted = race_df.sort_values('kakutei_chakujun')
        ranking = race_df_sorted['ketto_toroku_bango'].astype(str).tolist()
        
        if len(ranking) >= 3:
            rankings.append((race_id, ranking))
    
    print(f"âœ… Valid rankings: {len(rankings)}")
    
    # Power EP å­¦ç¿’
    model = PowerEPPlackettLuce(alpha=0.5)
    model.fit_power_ep(rankings, max_iter=50, tol=1e-4)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model.save("/home/user/eoi-pl/models/power_ep_pl_model.pkl")
    
    with open("/home/user/eoi-pl/models/power_ep_training_history.json", 'w') as f:
        json.dump(model.training_history, f, indent=2)
    
    # ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬
    print("\n" + "="*60)
    print("ğŸ”® Sample Prediction")
    print("="*60)
    
    sample_race_id, sample_ranking = rankings[0]
    probs = model.predict_proba_pl(sample_ranking)
    
    print(f"Race: {sample_race_id}")
    print(f"Horses: {len(sample_ranking)}")
    print("\nTop 5 predictions:")
    for i, (h, p) in enumerate(sorted(probs.items(), key=lambda x: x[1], reverse=True)[:5], 1):
        mu = model.skill_mu.get(h, 0.0)
        sigma = model.skill_sigma.get(h, 1.0)
        print(f"  {i}. Horse {h}: P(win)={p:.4f}, Î¼={mu:.3f}, Ïƒ={sigma:.3f}")
    
    print("\nâœ… Power EP training completed")
